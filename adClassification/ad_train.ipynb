{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ad_train.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"mwpbNzIWkTZo"},"source":["!git clone https://github.com/SKTBrain/KoBERT.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T2lCl5rs2LZY"},"source":["cd KoBERT"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJbSjWef4sza"},"source":["!pip install -r requirements.txt\n","!pip install .\n","!pip install torch\n","\n","# transformers 의 경우 최신 버전으로 설치 시 에러 발생\n","!pip install transformers #==3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JLaSXfDqjC3"},"source":["import os\n","import pandas as pd\n","import numpy as np\n","import csv \n","import sys\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch import nn\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","import gluonnlp as nlp\n","from torch.utils.data import Dataset, DataLoader\n","\n","# from kobert_transformers import get_kobert_model, get_distilkobert_model\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","from tqdm import tqdm, tqdm_notebook"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 환경 세팅"],"metadata":{"id":"SPhh6VD0GeVf"}},{"cell_type":"code","metadata":{"id":"mP6kOnq83hXZ"},"source":["# pytorch 에러 확인을 위한 패키지 버전 확인\n","print(sys.version)\n","print(\"Torch version:{}\".format(torch.__version__))\n","print(\"cuda version: {}\".format(torch.version.cuda))\n","print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GrpsK5SeY5aU"},"source":["#GPU 사용을 위해\n","device = torch.device(\"cuda:0\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nhn9InHuY5da"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x7mHZkhc-u0k"},"source":["# tokenizer"]},{"cell_type":"code","metadata":{"id":"U42bD1Iu-sqA"},"source":["# 이미 pre train 된 bert 모델과 vocab을 불러옴\n","bertmodel, vocab = get_pytorch_kobert_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M45_CuYoZkHF"},"source":["# 위에서 불러온 vocab을 이용하여 토크나이저 선언\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H58O5L8k-_Q9"},"source":["# bert 모델 최적화"]},{"cell_type":"code","metadata":{"id":"9xUanIuPrTbX"},"source":["# Setting parameters\n","max_len = 128 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 5\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate = 5e-5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ck1IWEWdau39"},"source":["# 학습할 때 사용할 데이터셋 클래스 선언\n","# list 대신 dataFrame을 이용했기 때문에 이에 맞게 매개 변수 등을 수정함\n","\n","# 기존 코드\n","# class BERTDataset(Dataset):\n","#     def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","#                  pad, pair):\n","#         transform = nlp.data.BERTSentenceTransform(\n","#             bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair) \n","\n","#         self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","#         self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","#     def __getitem__(self, i):\n","#         return (self.sentences[i] + (self.labels[i], ))\n","\n","#     def __len__(self):\n","#         return (len(self.labels))\n","\n","class BERTDataset(Dataset):\n","    # sentences 의 index와 label index 매개변수 제거\n","    def __init__(self, dataset, bert_tokenizer, max_len, pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair) \n","\n","      # sentences 리스트와 label 리스트를 만드는 코드 변경\n","        self.sentences = [transform([i]) for i in dataset['contents']]\n","        self.labels = list(dataset['label'])\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768, # Dimensionality of the encoder layers and the pooler layer\n","                 num_classes = 2, # 분류할 class label의 개수                \n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"],"metadata":{"id":"FHXN90JHHTNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)"],"metadata":{"id":"QRHuYaWdHkFl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]"],"metadata":{"id":"A_pen3G4Hoa4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"XCZRcq-iHodk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 데이터 로딩"],"metadata":{"id":"F7xvBF3hHWET"}},{"cell_type":"code","source":["train_data_path = \"/content/drive/MyDrive/forColab/train1_data.csv\"\n","train_data = pd.read_csv(train_data_path)\n","print('train data shape:', train_data.shape)\n","print('train data의 \\'contents\\' data type:', train_data['contents'].dtypes)\n","print('train data의 \\'label\\' data type:', train_data['label'].dtypes)\n","print('train data label:', train_data['label'].unique())"],"metadata":{"id":"RvhK5rzOHTOB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data_path = \"/content/drive/MyDrive/forColab/test_data.csv\"\n","test_data = pd.read_csv(test_data_path)\n","print('test data shape:', test_data.shape)\n","print('test data의 \\'contents\\' data type:', test_data['contents'].dtypes)\n","print('test data의 \\'label\\' data type:', test_data['label'].dtypes)\n","print('test data label:', test_data['label'].unique())\n","\n","# 오류있는 경우가 있어 추가\n","test_data = test_data[test_data['label'] != '#NAME?']\n","test_data['label'] = test_data['label'].astype('int')\n","print('test data의 \\'label\\' data type:', test_data['label'].dtypes)\n","print('test data label:', test_data['label'].unique())"],"metadata":{"id":"qIu0CvVTHTO9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data.head()"],"metadata":{"id":"jDzcwLIuHTQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data.head()"],"metadata":{"id":"Ap_W7YzTHTfV"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AH5LjakHa9kg"},"source":["train_dataset = BERTDataset(train_data, tok, max_len, True, False)\n","test_dataset = BERTDataset(test_data, tok, max_len, True, False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5kxYAnecbGp5"},"source":["train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=5, shuffle=True)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=5, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"afAWy9CwrW_5"},"source":["t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0tx-WLHir7bJ"},"source":["scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w5NwMhC6BGv2"},"source":["model_save_path = '/content/drive/MyDrive/forColab/learned_model/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8NNHHYMuusA0"},"source":["def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s7i0Sn4q1V5e"},"source":["# 학습\n","for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    model.train()\n","    \n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","        # optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        model.zero_grad()         # 그래디언트 초기화\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    \n","    model.eval()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        test_acc += calc_accuracy(out, label)\n","    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n","\n","    \n","    torch.save({\n","     'epoch' : e+1,\n","     'model_state_dict': model.state_dict(),\n","     'optimizer_state_dict': optimizer.state_dict()\n","     }, model_save_path + 'model_{}.tar'.format(e+1) )\n","    \n","\n","# 모델 저장~\n","torch.save(model, model_save_path + 'model3.pt')  # 전체 모델 저장\n","torch.save(model.state_dict(), model_save_path + 'model3_state_dict.pt')  # 모델 객체의 state_dict 저장"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hJGJxFbV0kNG"},"source":["label"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"wan2xxgUcPWq"},"execution_count":null,"outputs":[]}]}